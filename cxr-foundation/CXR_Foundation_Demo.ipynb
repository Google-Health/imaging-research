{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JIpyR7i-XtMy"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/Google-Health/imaging-research/blob/master/cxr-foundation/CXR_Foundation_Demo.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/Google-Health/imaging-research/blob/master/cxr-foundation/CXR_Foundation_Demo.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OTTGbllChDdY"
      },
      "source": [
        "# CXR Foundation Demo\n",
        "\n",
        "This notebook demonstrates the richness of information contained in embeddings, generated from full Chest X-Ray images. The contents include how to:\n",
        "\n",
        "- Download DICOM images and labels from the open-access NIH ChestX-ray14 dataset\n",
        "- Use the CXR Foundation API to generate image embeddings from the DICOMs\n",
        "- Train a simple neural network (WITHOUT needing GPU) to detect a medical finding in the embeddings\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "You must have access to use the CXR Foundation API. See the project's [README](https://github.com/Google-Health/imaging-research/blob/master/cxr-foundation/README.md) for details."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6Cbq7e2cEzU"
      },
      "source": [
        "# Installation\n",
        "\n",
        "Install the CXR Foundation package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAZeXrk9-u8R",
        "outputId": "191e71df-a423-4ffa-d4e8-1218d907098c"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Google-Health/imaging-research.git\n",
        "!pip install imaging-research/cxr-foundation/\n",
        "\n",
        "# Notebook specific dependencies\n",
        "!pip install matplotlib sklearn tf-models-official>=2.13.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_sNEw2facQnr"
      },
      "source": [
        "**IMPORTANT**: If you are using Colab, you must restart the runtime after installing new packages.\n",
        "\n",
        "NOTE: There will be some ERROR messages due to the protobuf library - this is normal."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Authenticate to Access Data\n",
        "\n",
        "The following cell is for Colab only. If running elsewhere, authenticate with the [gcloud CLI](https://cloud.google.com/sdk/gcloud/reference/auth/login)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "\n",
        "# Authenticate user for access. There will be a popup asking you to sign in with your user and approve access.\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xll2FURLgi4l"
      },
      "source": [
        "# Download Data\n",
        "\n",
        "The NIH ChestX-ray14 dataset, consists of over 100,000 de-identified images of chest x-rays, with fourteen common disease labels, text-mined from the text radiological reports via NLP techniques. The dataset is available on the NIH [download site](https://nihcc.app.box.com/v/ChestXray-NIHCC) and on [Google Cloud](https://cloud.google.com/healthcare-api/docs/resources/public-datasets/nih-chest).\n",
        "\n",
        "The CXR Foundation Demo GCS [bucket](https://console.cloud.google.com/storage/browser/cxr-foundation-demo) contains a subset of the data. We will download the dataset's labels file and some DICOM images below. This might take ~10 minutes or so depending on your connection speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Evn9WNBSd-CP",
        "outputId": "6d2049be-3aa0-498f-bf2f-d2c2f8092115"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import os\n",
        "\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "\n",
        "# Make a directory to download the data\n",
        "if not os.path.exists('data'):\n",
        "  os.mkdir('data')\n",
        "\n",
        "# Initialize the GCS storage client\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.get_bucket('cxr-foundation-demo')\n",
        "\n",
        "# Download and inspect the labels file.\n",
        "# There is a column for each of several findings, which indicate whether or not\n",
        "# the condition is present in the image file.\n",
        "full_labels_df = pd.read_csv(io.BytesIO(bucket.blob('cxr14/labels.csv').download_as_string()))\n",
        "# DICOM file paths on the data bucket\n",
        "full_labels_df['remote_dicom_file'] = full_labels_df['image_id'].apply(lambda x: os.path.join('cxr14', 'inputs', x.replace('.png', '.dcm')))\n",
        "display(full_labels_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the DICOM files, organized by the AIRSPACE_OPACITY label.\n",
        "# Download 100 of each case: positive and negative\n",
        "DIAGNOSIS = 'AIRSPACE_OPACITY' #@param {type: 'string'}\n",
        "MAX_CASES_PER_CATEGORY = 100 #@param {type: 'integer'}\n",
        "DICOM_DIR = './data/inputs' #@param {type: 'string'}\n",
        "EMBEDDINGS_DIR = './data/outputs' #@param {type: 'string'}\n",
        "\n",
        "# Labels df of relevant files\n",
        "df_labels = pd.concat((full_labels_df[full_labels_df[DIAGNOSIS]==0][:100],\n",
        "                      full_labels_df[full_labels_df[DIAGNOSIS]==1][:100]), ignore_index=True)\n",
        "df_labels = df_labels[[\"remote_dicom_file\", DIAGNOSIS]]\n",
        "\n",
        "# Path for downloaded DICOMs\n",
        "df_labels[\"dicom_file\"] = df_labels[\"remote_dicom_file\"].apply(\n",
        "    lambda x: os.path.join(DICOM_DIR, os.path.basename(x)))\n",
        "# Path for generated embeddings\n",
        "df_labels[\"embedding_file\"] =  df_labels['dicom_file'].apply(\n",
        "    lambda x: os.path.join(EMBEDDINGS_DIR, os.path.basename(x).replace(\".dcm\", \".tfrecord\")))\n",
        "\n",
        "df_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwp3bPfahDdj",
        "outputId": "2f3f3451-02f8-459e-bcf7-4f06552b37bc"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(DICOM_DIR):\n",
        "    os.makedirs(DICOM_DIR)\n",
        "\n",
        "for _, row in df_labels.iterrows():\n",
        "  blob = bucket.blob(row[\"remote_dicom_file\"])\n",
        "  if blob.exists():\n",
        "    blob.download_to_filename(row[\"dicom_file\"])\n",
        " \n",
        "print(\"Finished downloading DICOM files!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqdct5F0VFPw"
      },
      "source": [
        "# Generate Embeddings\n",
        "\n",
        "**IMPORTANT:** You must have access to use the CXR Foundation API. See the project's [README](https://github.com/Google-Health/imaging-research/blob/master/cxr-foundation/README.md) for details.\n",
        "\n",
        "Generate embeddings (think of them as compressed images) from the downloaded DICOMs. This may take ~15 minutes depending on the load on the server and your connection speed.\n",
        "\n",
        "*There may be some warnings about \"Could not load dynamic library\" and or \"No project ID could be determined,\" but these can be safely ignored.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists(EMBEDDINGS_DIR):\n",
        "  os.makedirs(EMBEDDINGS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8liPYf1JGdMO",
        "outputId": "c24fd68a-6fcd-4e7a-fa9f-fbae4b96531e"
      },
      "outputs": [],
      "source": [
        "!python -m cxr_foundation.run_inference \\\n",
        "  --input_path \"{DICOM_DIR}/*.dcm\" \\\n",
        "  --output_path \"{EMBEDDINGS_DIR}\" \\\n",
        "  --input_file_type='dicom' \\\n",
        "  --limit -1  # setting this flag to -1 means unlimited"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXPwZR3wHBEn",
        "outputId": "9f24976e-736a-4c66-d294-950fbf840127"
      },
      "outputs": [],
      "source": [
        "# Inspect some embeddings files. A single file is only 5.6kb\n",
        "from cxr_foundation import embeddings_data\n",
        "\n",
        "\n",
        "filename = glob.glob(os.path.join(EMBEDDINGS_DIR, '*.tfrecord'))[0]\n",
        "\n",
        "# Read the tf.train.Example object from the first tfrecord file\n",
        "example = embeddings_data.read_record_example(filename)\n",
        "print(example)\n",
        "\n",
        "# If you don't care about the structure of the .tfrecord file, and/or if\n",
        "# you don't use Tensorflow, just use the following function to read\n",
        "# the values directly into a numpy array.\n",
        "values = embeddings_data.read_record_values(filename)\n",
        "print(values)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QoHZSZO_hDdl"
      },
      "source": [
        "# Prepare Data for Model Training\n",
        "\n",
        "Separate into training, validation, and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Wr8tsaTLNpEH",
        "outputId": "dc6b0d7d-7e67-4149-dd86-2726626fd739"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "df_train, df_validate = train_test_split(df_labels, test_size=0.1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fVGYhxEkWBhs"
      },
      "source": [
        "# Train A Model\n",
        "\n",
        "Finally, we can train a model using the embeddings! With a simple feed-forward neural network, it should take < 5 minutes to train 100 epochs! No GPU required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_models as tfm\n",
        "\n",
        "\n",
        "def create_model(heads,\n",
        "                 embeddings_size=1376,\n",
        "                 learning_rate=0.1,\n",
        "                 end_lr_factor=1.0,\n",
        "                 dropout=0.0,\n",
        "                 decay_steps=1000,\n",
        "                 loss_weights=None,\n",
        "                 hidden_layer_sizes=[512, 256],\n",
        "                 weight_decay=0.0,\n",
        "                 seed=None) -> tf.keras.model:\n",
        "  \"\"\"\n",
        "  Creates linear probe or multilayer perceptron using LARS + cosine decay.\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  inputs = tf.keras.Input(shape=(embeddings_size,))\n",
        "  hidden = inputs\n",
        "  # If no hidden_layer_sizes are provided, model will be a linear probe.\n",
        "  for size in hidden_layer_sizes:\n",
        "    hidden = tf.keras.layers.Dense(\n",
        "        size,\n",
        "        activation='relu',\n",
        "        kernel_initializer=tf.keras.initializers.HeUniform(seed=seed),\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(l2=weight_decay),\n",
        "        bias_regularizer=tf.keras.regularizers.l2(l2=weight_decay))(\n",
        "            hidden)\n",
        "    hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
        "    hidden = tf.keras.layers.Dropout(dropout, seed=seed)(hidden)\n",
        "  output = tf.keras.layers.Dense(\n",
        "      units=len(heads),\n",
        "      activation='sigmoid',\n",
        "      kernel_initializer=tf.keras.initializers.HeUniform(seed=seed))(\n",
        "          hidden)\n",
        "\n",
        "  outputs = {}\n",
        "  for i, head in enumerate(heads):\n",
        "    outputs[head] = tf.keras.layers.Lambda(\n",
        "        lambda x: x[..., i:i + 1], name=head.lower())(\n",
        "            output)\n",
        "\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  learning_rate_fn = tf.keras.experimental.CosineDecay(\n",
        "      tf.cast(learning_rate, tf.float32),\n",
        "      tf.cast(decay_steps, tf.float32),\n",
        "      alpha=tf.cast(end_lr_factor, tf.float32))\n",
        "  model.compile(\n",
        "      optimizer=tfm.optimization.lars.LARS(\n",
        "          learning_rate=learning_rate_fn),\n",
        "      loss=dict([(head, 'binary_crossentropy') for head in heads]),\n",
        "      loss_weights=loss_weights or dict([(head, 1.) for head in heads]),\n",
        "      weighted_metrics=[\n",
        "        tf.keras.metrics.FalsePositives(),\n",
        "        tf.keras.metrics.FalseNegatives(),\n",
        "        tf.keras.metrics.TruePositives(),\n",
        "        tf.keras.metrics.TrueNegatives(),\n",
        "        tf.keras.metrics.AUC(),\n",
        "        tf.keras.metrics.AUC(curve='PR', name='auc_pr')])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training and validation Datasets\n",
        "training_data = embeddings_data.get_dataset(filenames=df_train[\"embedding_file\"].values,\n",
        "                        labels=df_train[DIAGNOSIS].values)\n",
        "\n",
        "\n",
        "validation_data = embeddings_data.get_dataset(filenames=df_validate[\"embedding_file\"].values,\n",
        "                        labels=df_validate[DIAGNOSIS].values)\n",
        "\n",
        "# Create and train the model\n",
        "model = create_model([DIAGNOSIS])\n",
        "\n",
        "model.fit(\n",
        "    x=training_data.batch(512).prefetch(tf.data.AUTOTUNE).cache(),\n",
        "    validation_data=validation_data.batch(1).cache(),\n",
        "    epochs=100,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUrd2ApY3NVH",
        "outputId": "c1bd3ff7-929b-4ea9-baba-7b644ef8b237"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oNpdUNuqyWk9"
      },
      "source": [
        "# Examine metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxr52PYuybZp"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_curve(x, y, auc, x_label=None, y_label=None, label=None):\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  plt.plot(x, y, label=f'{label} (AUC: %.3f)' % auc, color='black')\n",
        "  plt.legend(loc='lower right', fontsize=18)\n",
        "  plt.xlim([-0.01, 1.01])\n",
        "  plt.ylim([-0.01, 1.01])\n",
        "  if x_label:\n",
        "    plt.xlabel(x_label, fontsize=24)\n",
        "  if y_label:\n",
        "    plt.ylabel(y_label, fontsize=24)\n",
        "  plt.xticks(fontsize=12)\n",
        "  plt.yticks(fontsize=12)\n",
        "  plt.grid(visible=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6q-A_H39qY3"
      },
      "outputs": [],
      "source": [
        "rows = []\n",
        "for embeddings, label in validation_data.batch(1):\n",
        "  row = {\n",
        "      f'{DIAGNOSIS}_prediction': model(embeddings)[DIAGNOSIS].numpy().flatten()[0],\n",
        "      f'{DIAGNOSIS}_value': label.numpy().flatten()[0]\n",
        "  }\n",
        "  rows.append(row)\n",
        "eval_df = pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JXgsYY8emnT8",
        "outputId": "7f17ea32-deb3-4762-9c8c-d59a8b1deaf4"
      },
      "outputs": [],
      "source": [
        "eval_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "id": "Eh8MKqF8mdjp",
        "outputId": "52755c86-926f-478c-9c0b-058272397fbf"
      },
      "outputs": [],
      "source": [
        "labels = eval_df[f'{DIAGNOSIS}_value'].values\n",
        "predictions = eval_df[f'{DIAGNOSIS}_prediction'].values\n",
        "false_positive_rate, true_positive_rate, thresholds = sklearn.metrics.roc_curve(\n",
        "    labels,\n",
        "    predictions,\n",
        "    drop_intermediate=False)\n",
        "auc = sklearn.metrics.roc_auc_score(labels, predictions)\n",
        "plot_curve(false_positive_rate, true_positive_rate, auc, x_label='False Positive Rate', y_label='True Positive Rate', label=DIAGNOSIS)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8LpEO7UrU9eS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "d3ac608b8f9188be2227ae82298dfd5de684cbdc4496f362d4b3b9040509447c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
