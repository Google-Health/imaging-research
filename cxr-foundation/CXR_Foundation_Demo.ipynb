{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIpyR7i-XtMy"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/Google-Health/imaging-research/blob/master/cxr-foundation/CXR_Foundation_Demo.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/Google-Health/imaging-research/blob/master/cxr-foundation/CXR_Foundation_Demo.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTTGbllChDdY"
      },
      "source": [
        "# CXR Foundation Demo\n",
        "\n",
        "This notebook demonstrates the richness of information contained in embeddings, generated from full Chest X-Ray images. The contents include how to:\n",
        "\n",
        "- Download DICOM images and labels from the open-access NIH ChestX-ray14 dataset\n",
        "- Use the CXR Foundation API to generate image embeddings from the DICOMs\n",
        "- Train a simple neural network (WITHOUT needing GPU) to detect a medical finding in the embeddings\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "You must have access to use the CXR Foundation API. See the project's [README](https://github.com/Google-Health/imaging-research/blob/master/cxr-foundation/README.md) for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6Cbq7e2cEzU"
      },
      "source": [
        "# Installation\n",
        "\n",
        "Install the CXR Foundation package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAZeXrk9-u8R"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Google-Health/imaging-research.git\n",
        "!pip install imaging-research/cxr-foundation/\n",
        "\n",
        "# Notebook specific dependencies\n",
        "!pip install matplotlib sklearn tf-models-official>=2.13.0 google-cloud-storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sNEw2facQnr"
      },
      "source": [
        "**IMPORTANT**: If you are using Colab, you must restart the runtime after installing new packages.\n",
        "\n",
        "NOTE: There will be some ERROR messages due to the protobuf library - this is normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBMLFqT4cyyk"
      },
      "source": [
        "# Authenticate to Access Data\n",
        "\n",
        "The following cell is for Colab only. If running elsewhere, authenticate with the [gcloud CLI](https://cloud.google.com/sdk/gcloud/reference/auth/login)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t2zwPufcyyl"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "\n",
        "# Authenticate user for access. There will be a popup asking you to sign in with your user and approve access.\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xll2FURLgi4l"
      },
      "source": [
        "# Download Data\n",
        "\n",
        "The NIH ChestX-ray14 dataset, consists of over 100,000 de-identified images of chest x-rays, with fourteen common disease labels, text-mined from the text radiological reports via NLP techniques. The dataset is available on the NIH [download site](https://nihcc.app.box.com/v/ChestXray-NIHCC) and on [Google Cloud](https://cloud.google.com/healthcare-api/docs/resources/public-datasets/nih-chest).\n",
        "\n",
        "The CXR Foundation Demo GCS [bucket](https://console.cloud.google.com/storage/browser/cxr-foundation-demo) contains a subset of the data. We will download the dataset's labels file and some DICOM images below. This might take ~10 minutes or so depending on your connection speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Evn9WNBSd-CP"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "\n",
        "# Make a directory to download the data\n",
        "if not os.path.exists('data'):\n",
        "  os.mkdir('data')\n",
        "\n",
        "# Initialize the GCS storage client\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.get_bucket('cxr-foundation-demo')\n",
        "\n",
        "# Download and inspect the labels file.\n",
        "# There is a column for each of several findings, which indicate whether or not\n",
        "# the condition is present in the image file.\n",
        "full_labels_df = pd.read_csv(io.BytesIO(bucket.blob('cxr14/labels.csv').download_as_string()))\n",
        "# DICOM file paths on the data bucket\n",
        "full_labels_df['remote_dicom_file'] = full_labels_df['image_id'].apply(lambda x: os.path.join('cxr14', 'inputs', x.replace('.png', '.dcm')))\n",
        "display(full_labels_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8swJLqDcyyl"
      },
      "source": [
        "Download the DICOM files, organized by the AIRSPACE_OPACITY label.\n",
        "\n",
        "Download 100 of each case: positive and negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiFTgj8Icyyl"
      },
      "outputs": [],
      "source": [
        "DIAGNOSIS = 'AIRSPACE_OPACITY' #@param {type: 'string'}\n",
        "MAX_CASES_PER_CATEGORY = 100 #@param {type: 'integer'}\n",
        "DICOM_DIR = './data/inputs' #@param {type: 'string'}\n",
        "EMBEDDINGS_DIR = './data/outputs' #@param {type: 'string'}\n",
        "\n",
        "# Labels df of relevant files\n",
        "df_labels = pd.concat((full_labels_df[full_labels_df[DIAGNOSIS]==0][:MAX_CASES_PER_CATEGORY],\n",
        "                      full_labels_df[full_labels_df[DIAGNOSIS]==1][:MAX_CASES_PER_CATEGORY]), ignore_index=True)\n",
        "df_labels = df_labels[[\"remote_dicom_file\", DIAGNOSIS]]\n",
        "\n",
        "# Path for downloaded DICOMs\n",
        "df_labels[\"dicom_file\"] = df_labels[\"remote_dicom_file\"].apply(\n",
        "    lambda x: os.path.join(DICOM_DIR, os.path.basename(x)))\n",
        "# Path for generated embeddings\n",
        "df_labels[\"embedding_file\"] =  df_labels['dicom_file'].apply(\n",
        "    lambda x: os.path.join(EMBEDDINGS_DIR, os.path.basename(x).replace(\".dcm\", \".tfrecord\")))\n",
        "\n",
        "df_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwp3bPfahDdj"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(DICOM_DIR):\n",
        "    os.makedirs(DICOM_DIR)\n",
        "\n",
        "for _, row in df_labels.iterrows():\n",
        "  blob = bucket.blob(row[\"remote_dicom_file\"])\n",
        "  if blob.exists():\n",
        "    blob.download_to_filename(row[\"dicom_file\"])\n",
        "print(\"Finished downloading DICOM files!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqdct5F0VFPw"
      },
      "source": [
        "# Generate Embeddings\n",
        "\n",
        "**IMPORTANT:** You must have access to use the CXR Foundation API. See the project's [README](https://github.com/Google-Health/imaging-research/blob/master/cxr-foundation/README.md) for details.\n",
        "\n",
        "Generate embeddings (think of them as compressed images) from the downloaded DICOMs. This may take ~15 minutes depending on the load on the server and your connection speed.\n",
        "\n",
        "*There may be some warnings about \"Could not load dynamic library\" and or \"No project ID could be determined,\" but these can be safely ignored.*\n",
        "\n",
        "## Storage Format\n",
        "\n",
        "The cxr-foundation library supports storing generated values in both .npz and .tfrecord format.\n",
        "\n",
        "The following cells demonstrate how to do both. The subsequent model training section will only use the .tfrecord files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r1YZDcEcyym"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(EMBEDDINGS_DIR):\n",
        "  os.makedirs(EMBEDDINGS_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional\n",
        "Check DICOM files to make sure image data appears valid."
      ],
      "metadata": {
        "id": "P1SVmK5HCCfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "max_index = len(df_labels[\"dicom_file\"].values)\n",
        "print('There are total of %s images. We will sample 5 to display.' % max_index)\n",
        "list_of_images = random.sample(range(0, max_index-1), 5)\n",
        "\n",
        "import io\n",
        "import pydicom\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from cxr_foundation import constants\n",
        "from cxr_foundation import example_generator_lib\n",
        "\n",
        "file_to_visualize = 2\n",
        "\n",
        "def show_dicom(adicom):\n",
        "  \"\"\"Shows the DICOM in a format as passed to CXR Foundation.\"\"\"\n",
        "  png_image_data = example.features.feature[\n",
        "      constants.IMAGE_KEY].bytes_list.value[:][0]\n",
        "  image = Image.open(io.BytesIO(png_image_data))\n",
        "  figure_size=7\n",
        "  f, axarr = plt.subplots(1, 1, figsize = (figure_size, figure_size))\n",
        "  axarr.imshow(image, cmap='gray')\n",
        "  axarr.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "for image_idx in list_of_images:\n",
        "  dicom = pydicom.read_file(df_labels[\"dicom_file\"].values[image_idx])\n",
        "  example = example_generator_lib.dicom_to_tfexample(dicom)\n",
        "  print('Image %d' % image_idx)\n",
        "  show_dicom(example)"
      ],
      "metadata": {
        "id": "l4-3qMMHCKQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8liPYf1JGdMO"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "from cxr_foundation.inference import generate_embeddings, InputFileType, OutputFileType\n",
        "\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Generate and store a few embeddings in .npz format\n",
        "generate_embeddings(input_files=df_labels[\"dicom_file\"].values[:5], output_dir=EMBEDDINGS_DIR,\n",
        "    input_type=InputFileType.DICOM, output_type=OutputFileType.NPZ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw_8isYTcyym"
      },
      "outputs": [],
      "source": [
        "from cxr_foundation import embeddings_data\n",
        "\n",
        "# Read the data from a generated .npz embeddings file.\n",
        "filename = df_labels[\"embedding_file\"][0].replace(\"tfrecord\", \"npz\")\n",
        "values = embeddings_data.read_npz_values(filename)\n",
        "\n",
        "print(values)\n",
        "\n",
        "# NOTE: The rest of the notebook will use the .tfrecord data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE55-FOMcyym"
      },
      "outputs": [],
      "source": [
        "# Generate all the embedings in .tfrecord format\n",
        "generate_embeddings(input_files=df_labels[\"dicom_file\"].values, output_dir=EMBEDDINGS_DIR,\n",
        "    input_type=InputFileType.DICOM, output_type=OutputFileType.TFRECORD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXPwZR3wHBEn"
      },
      "outputs": [],
      "source": [
        "# Inspect a tfrecord embedding file. A single file is only several kbs.\n",
        "filename = df_labels[\"embedding_file\"][0]\n",
        "\n",
        "# Read the tf.train.Example object from the first tfrecord file\n",
        "example = embeddings_data.read_tfrecord_example(filename)\n",
        "print(example)\n",
        "\n",
        "# If you don't care about the structure of the .tfrecord file, and/or if\n",
        "# you don't use Tensorflow, you can use the following function to read\n",
        "# the values directly into a numpy array.\n",
        "values = embeddings_data.read_tfrecord_values(filename)\n",
        "print(values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoHZSZO_hDdl"
      },
      "source": [
        "# Prepare Data for Model Training\n",
        "\n",
        "Separate into training, validation, and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr8tsaTLNpEH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "df_train, df_validate = train_test_split(df_labels, test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVGYhxEkWBhs"
      },
      "source": [
        "# Train A Model\n",
        "\n",
        "Finally, we can train a model using the embeddings! With a simple feed-forward neural network, it should take < 5 minutes to train 100 epochs! No GPU required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6KeAAEJcyym"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_models as tfm\n",
        "\n",
        "\n",
        "def create_model(heads,\n",
        "                 embeddings_size=1376,\n",
        "                 learning_rate=0.1,\n",
        "                 end_lr_factor=1.0,\n",
        "                 dropout=0.0,\n",
        "                 decay_steps=1000,\n",
        "                 loss_weights=None,\n",
        "                 hidden_layer_sizes=[512, 256],\n",
        "                 weight_decay=0.0,\n",
        "                 seed=None) -> tf.keras.Model:\n",
        "  \"\"\"\n",
        "  Creates linear probe or multilayer perceptron using LARS + cosine decay.\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  inputs = tf.keras.Input(shape=(embeddings_size,))\n",
        "  hidden = inputs\n",
        "  # If no hidden_layer_sizes are provided, model will be a linear probe.\n",
        "  for size in hidden_layer_sizes:\n",
        "    hidden = tf.keras.layers.Dense(\n",
        "        size,\n",
        "        activation='relu',\n",
        "        kernel_initializer=tf.keras.initializers.HeUniform(seed=seed),\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(l2=weight_decay),\n",
        "        bias_regularizer=tf.keras.regularizers.l2(l2=weight_decay))(\n",
        "            hidden)\n",
        "    hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
        "    hidden = tf.keras.layers.Dropout(dropout, seed=seed)(hidden)\n",
        "  output = tf.keras.layers.Dense(\n",
        "      units=len(heads),\n",
        "      activation='sigmoid',\n",
        "      kernel_initializer=tf.keras.initializers.HeUniform(seed=seed))(\n",
        "          hidden)\n",
        "\n",
        "  outputs = {}\n",
        "  for i, head in enumerate(heads):\n",
        "    outputs[head] = tf.keras.layers.Lambda(\n",
        "        lambda x: x[..., i:i + 1], name=head.lower())(\n",
        "            output)\n",
        "\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  learning_rate_fn = tf.keras.experimental.CosineDecay(\n",
        "      tf.cast(learning_rate, tf.float32),\n",
        "      tf.cast(decay_steps, tf.float32),\n",
        "      alpha=tf.cast(end_lr_factor, tf.float32))\n",
        "  model.compile(\n",
        "      optimizer=tfm.optimization.lars.LARS(\n",
        "          learning_rate=learning_rate_fn),\n",
        "      loss=dict([(head, 'binary_crossentropy') for head in heads]),\n",
        "      loss_weights=loss_weights or dict([(head, 1.) for head in heads]),\n",
        "      weighted_metrics=[\n",
        "        tf.keras.metrics.FalsePositives(),\n",
        "        tf.keras.metrics.FalseNegatives(),\n",
        "        tf.keras.metrics.TruePositives(),\n",
        "        tf.keras.metrics.TrueNegatives(),\n",
        "        tf.keras.metrics.AUC(),\n",
        "        tf.keras.metrics.AUC(curve='PR', name='auc_pr')])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0UbFRBncyym"
      },
      "outputs": [],
      "source": [
        "# Create training and validation Datasets\n",
        "training_data = embeddings_data.get_dataset(filenames=df_train[\"embedding_file\"].values,\n",
        "                        labels=df_train[DIAGNOSIS].values)\n",
        "\n",
        "\n",
        "validation_data = embeddings_data.get_dataset(filenames=df_validate[\"embedding_file\"].values,\n",
        "                        labels=df_validate[DIAGNOSIS].values)\n",
        "\n",
        "# Create and train the model\n",
        "model = create_model([DIAGNOSIS])\n",
        "\n",
        "model.fit(\n",
        "    x=training_data.batch(512).prefetch(tf.data.AUTOTUNE).cache(),\n",
        "    validation_data=validation_data.batch(1).cache(),\n",
        "    epochs=100,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUrd2ApY3NVH"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNpdUNuqyWk9"
      },
      "source": [
        "# Examine metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxr52PYuybZp"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_curve(x, y, auc, x_label=None, y_label=None, label=None):\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  plt.plot(x, y, label=f'{label} (AUC: %.3f)' % auc, color='black')\n",
        "  plt.legend(loc='lower right', fontsize=18)\n",
        "  plt.xlim([-0.01, 1.01])\n",
        "  plt.ylim([-0.01, 1.01])\n",
        "  if x_label:\n",
        "    plt.xlabel(x_label, fontsize=24)\n",
        "  if y_label:\n",
        "    plt.ylabel(y_label, fontsize=24)\n",
        "  plt.xticks(fontsize=12)\n",
        "  plt.yticks(fontsize=12)\n",
        "  plt.grid(visible=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6q-A_H39qY3"
      },
      "outputs": [],
      "source": [
        "rows = []\n",
        "for embeddings, label in validation_data.batch(1):\n",
        "  row = {\n",
        "      f'{DIAGNOSIS}_prediction': model(embeddings)[DIAGNOSIS].numpy().flatten()[0],\n",
        "      f'{DIAGNOSIS}_value': label.numpy().flatten()[0]\n",
        "  }\n",
        "  rows.append(row)\n",
        "eval_df = pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXgsYY8emnT8"
      },
      "outputs": [],
      "source": [
        "eval_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eh8MKqF8mdjp"
      },
      "outputs": [],
      "source": [
        "labels = eval_df[f'{DIAGNOSIS}_value'].values\n",
        "predictions = eval_df[f'{DIAGNOSIS}_prediction'].values\n",
        "false_positive_rate, true_positive_rate, thresholds = sklearn.metrics.roc_curve(\n",
        "    labels,\n",
        "    predictions,\n",
        "    drop_intermediate=False)\n",
        "auc = sklearn.metrics.roc_auc_score(labels, predictions)\n",
        "plot_curve(false_positive_rate, true_positive_rate, auc, x_label='False Positive Rate', y_label='True Positive Rate', label=DIAGNOSIS)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8LpEO7UrU9eS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "d3ac608b8f9188be2227ae82298dfd5de684cbdc4496f362d4b3b9040509447c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}