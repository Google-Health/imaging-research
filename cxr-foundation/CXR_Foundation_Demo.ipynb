{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIpyR7i-XtMy"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/Google-Health/imaging-research/blob/master/cxr-foundation/CXR_Foundation_Demo.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/Google-Health/imaging-research/blob/master/cxr-foundation/CXR_Foundation_Demo.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTTGbllChDdY"
      },
      "source": [
        "# CXR Foundation Demo\n",
        "\n",
        "This notebook demonstrates the richness of information contained in embeddings, generated from full Chest X-Ray images. The contents include how to:\n",
        "\n",
        "- Download DICOM images and labels from the open-access NIH ChestX-ray14 dataset\n",
        "- Use the CXR Foundation API to generate image embeddings from the DICOMs\n",
        "- Train a simple neural network (WITHOUT needing GPU) to detect a medical finding in the embeddings\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "You must have access to use the CXR Foundation API. See the project's [README](https://github.com/Google-Health/imaging-research/blob/master/cxr-foundation/README.md) for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6Cbq7e2cEzU"
      },
      "source": [
        "# Installation\n",
        "\n",
        "Install the CXR Foundation package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAZeXrk9-u8R"
      },
      "outputs": [],
      "source": [
        "# Notebook specific dependencies\n",
        "!pip install matplotlib tf-models-official==2.14.0 google-cloud-storage\n",
        "\n",
        "!git clone https://github.com/Google-Health/imaging-research.git\n",
        "!pip install imaging-research/cxr-foundation/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sNEw2facQnr"
      },
      "source": [
        "**IMPORTANT**: If you are using Colab, you must restart the runtime after installing new packages.\n",
        "\n",
        "NOTE: There will be some ERROR messages due to the protobuf library - this is normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBMLFqT4cyyk"
      },
      "source": [
        "# Authenticate to Access Data\n",
        "\n",
        "The following cell is for Colab only. If running elsewhere, authenticate with the [gcloud CLI](https://cloud.google.com/sdk/gcloud/reference/auth/login)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t2zwPufcyyl"
      },
      "outputs": [],
      "source": [
        "USE_SERVICE_ACCOUNT = True #@param {type: 'boolean'}\n",
        "\n",
        "from google.colab import auth\n",
        "\n",
        "# Authenticate user for access.\n",
        "if USE_SERVICE_ACCOUNT:\n",
        "  # There will be a prompt below asking you to upload your service account key as a JSON file.\n",
        "  auth.authenticate_service_account()\n",
        "else:\n",
        "  # There will be a popup asking you to sign in with your user and approve access.\n",
        "  auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xll2FURLgi4l"
      },
      "source": [
        "# Download Data\n",
        "\n",
        "The NIH ChestX-ray14 dataset, consists of over 100,000 de-identified images of chest x-rays, with fourteen common disease labels, text-mined from the text radiological reports via NLP techniques. The dataset is available on the NIH [download site](https://nihcc.app.box.com/v/ChestXray-NIHCC) and on [Google Cloud](https://cloud.google.com/healthcare-api/docs/resources/public-datasets/nih-chest).\n",
        "\n",
        "The CXR Foundation Demo GCS [bucket](https://console.cloud.google.com/storage/browser/cxr-foundation-demo) contains a subset of the data. We will download the dataset's labels file and some DICOM images below. This might take ~10 minutes or so depending on your connection speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Evn9WNBSd-CP"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "\n",
        "# Make a directory to download the data\n",
        "if not os.path.exists('data'):\n",
        "  os.mkdir('data')\n",
        "\n",
        "# Initialize the GCS storage client\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.get_bucket('cxr-foundation-demo')\n",
        "\n",
        "# Download and inspect the labels file.\n",
        "# There is a column for each of several findings, which indicate whether or not\n",
        "# the condition is present in the image file.\n",
        "full_labels_df = pd.read_csv(io.BytesIO(bucket.blob('cxr14/labels.csv').download_as_string()))\n",
        "# DICOM file paths on the data bucket\n",
        "full_labels_df['remote_dicom_file'] = full_labels_df['image_id'].apply(lambda x: os.path.join('cxr14', 'inputs', x.replace('.png', '.dcm')))\n",
        "display(full_labels_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8swJLqDcyyl"
      },
      "source": [
        "Download the DICOM files, organized by the AIRSPACE_OPACITY label.\n",
        "\n",
        "Download 100 of each case: positive and negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiFTgj8Icyyl",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "DIAGNOSIS = 'AIRSPACE_OPACITY' #@param {type: 'string'}\n",
        "MAX_CASES_PER_CATEGORY = 100 #@param {type: 'integer'}\n",
        "DICOM_DIR = './data/inputs' #@param {type: 'string'}\n",
        "EMBEDDINGS_DIR = './data/outputs' #@param {type: 'string'}\n",
        "\n",
        "# Labels df of relevant files\n",
        "df_labels = pd.concat((full_labels_df[full_labels_df[DIAGNOSIS]==0][:MAX_CASES_PER_CATEGORY],\n",
        "                      full_labels_df[full_labels_df[DIAGNOSIS]==1][:MAX_CASES_PER_CATEGORY]), ignore_index=True)\n",
        "df_labels = df_labels[[\"remote_dicom_file\", DIAGNOSIS]]\n",
        "\n",
        "# Path for downloaded DICOMs\n",
        "df_labels[\"dicom_file\"] = df_labels[\"remote_dicom_file\"].apply(\n",
        "    lambda x: os.path.join(DICOM_DIR, os.path.basename(x)))\n",
        "# Path for generated embeddings\n",
        "df_labels[\"embedding_file\"] =  df_labels['dicom_file'].apply(\n",
        "    lambda x: os.path.join(EMBEDDINGS_DIR, os.path.basename(x).replace(\".dcm\", \".tfrecord\")))\n",
        "\n",
        "df_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwp3bPfahDdj"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(DICOM_DIR):\n",
        "    os.makedirs(DICOM_DIR)\n",
        "\n",
        "for _, row in df_labels.iterrows():\n",
        "  blob = bucket.blob(row[\"remote_dicom_file\"])\n",
        "  if blob.exists():\n",
        "    blob.download_to_filename(row[\"dicom_file\"])\n",
        "print(\"Finished downloading DICOM files!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util functions and imports"
      ],
      "metadata": {
        "id": "-e9SeKUzTnSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import pydicom\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from cxr_foundation import constants\n",
        "from cxr_foundation import example_generator_lib\n",
        "\n",
        "def show_dicom(adicom):\n",
        "  \"\"\"Shows the DICOM in a format as passed to CXR Foundation.\"\"\"\n",
        "  png_image_data = example.features.feature[\n",
        "      constants.IMAGE_KEY].bytes_list.value[:][0]\n",
        "  image = Image.open(io.BytesIO(png_image_data))\n",
        "  figure_size=7\n",
        "  f, axarr = plt.subplots(1, 1, figsize = (figure_size, figure_size))\n",
        "  axarr.imshow(image, cmap='gray')\n",
        "  axarr.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "o6a7eLW3Tmlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqdct5F0VFPw"
      },
      "source": [
        "# Generate Embeddings\n",
        "\n",
        "**IMPORTANT:** You must have access to use the CXR Foundation API. See the project's [README](https://github.com/Google-Health/imaging-research/blob/master/cxr-foundation/README.md) for details.\n",
        "\n",
        "Generate embeddings (think of them as compressed images) from the downloaded DICOMs. This may take ~15 minutes depending on the load on the server and your connection speed.\n",
        "\n",
        "*There may be some warnings about \"Could not load dynamic library\" and or \"No project ID could be determined,\" but these can be safely ignored.*\n",
        "\n",
        "## Storage Format\n",
        "\n",
        "The cxr-foundation library supports storing generated values in both .npz and .tfrecord format.\n",
        "\n",
        "The following cells demonstrate how to do both. The subsequent model training section will only use the .tfrecord files.\n",
        "\n",
        "## Embedding Version\n",
        "\n",
        "We support the following three embedding versions:\n",
        "- cxr_foundation: the original CXR foundation embedding\n",
        "- elixr: the raw image embedding from the Q-former output in ELIXR (https://arxiv.org/abs/2308.01317), can be used for data-efficient classification (same as CXR foundation embedding)\n",
        "- elixr_img_contrastive: the text-aligned image embedding from the Q-former output in ELIXR (https://arxiv.org/abs/2308.01317), can be used for image retrieval. Refer to \"Image Retrieval Demo\" section in this colab for example usage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selection between embedding versions"
      ],
      "metadata": {
        "id": "ru_dJOFliER2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cxr_foundation.inference import ModelVersion\n",
        "import shutil\n",
        "\n",
        "EMBEDDING_VERSION = 'elixr' #@param ['elixr', 'cxr_foundation', 'elixr_img_contrastive']\n",
        "if EMBEDDING_VERSION == 'cxr_foundation':\n",
        "  MODEL_VERSION = ModelVersion.V1\n",
        "  TOKEN_NUM = 1\n",
        "  EMBEDDINGS_SIZE = 1376\n",
        "elif EMBEDDING_VERSION == 'elixr':\n",
        "  MODEL_VERSION = ModelVersion.V2\n",
        "  TOKEN_NUM = 32\n",
        "  EMBEDDINGS_SIZE = 768\n",
        "elif EMBEDDING_VERSION == 'elixr_img_contrastive':\n",
        "  MODEL_VERSION = ModelVersion.V2_CONTRASTIVE\n",
        "  TOKEN_NUM = 32\n",
        "  EMBEDDINGS_SIZE = 128\n",
        "if not os.path.exists(EMBEDDINGS_DIR):\n",
        "  os.makedirs(EMBEDDINGS_DIR)\n",
        "else:\n",
        "  # Empty embedding dir to avoid caching when switching embedding versions\n",
        "  shutil.rmtree(EMBEDDINGS_DIR)\n",
        "  os.makedirs(EMBEDDINGS_DIR)"
      ],
      "metadata": {
        "id": "53heuYYCh1x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional (test run for a few cases)"
      ],
      "metadata": {
        "id": "P1SVmK5HCCfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check DICOM files to make sure image data appears valid."
      ],
      "metadata": {
        "id": "tj3oogu_JmOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "max_index = len(df_labels[\"dicom_file\"].values)\n",
        "file_to_visualize = 2\n",
        "print('There are total of %s images. We will sample %d to display.' % (max_index, file_to_visualize))\n",
        "list_of_images = random.sample(range(0, max_index-1), file_to_visualize)\n",
        "\n",
        "for image_idx in list_of_images:\n",
        "  dicom = pydicom.read_file(df_labels[\"dicom_file\"].values[image_idx])\n",
        "  example = example_generator_lib.dicom_to_tfexample(dicom)\n",
        "  print('Image %d' % image_idx)\n",
        "  show_dicom(example)"
      ],
      "metadata": {
        "id": "l4-3qMMHCKQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try inference on 5 cases"
      ],
      "metadata": {
        "id": "GwbA6Y61JrKe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8liPYf1JGdMO"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "from cxr_foundation.inference import generate_embeddings, InputFileType, OutputFileType, ModelVersion\n",
        "\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Generate and store a few embeddings in .npz format\n",
        "generate_embeddings(input_files=df_labels[\"dicom_file\"].values[:5], output_dir=EMBEDDINGS_DIR,\n",
        "    input_type=InputFileType.DICOM, output_type=OutputFileType.NPZ, model_version=MODEL_VERSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw_8isYTcyym"
      },
      "outputs": [],
      "source": [
        "from cxr_foundation import embeddings_data\n",
        "\n",
        "# Read the data from a generated .npz embeddings file.\n",
        "filename = df_labels[\"embedding_file\"][0].replace(\"tfrecord\", \"npz\")\n",
        "values = embeddings_data.read_npz_values(filename)\n",
        "\n",
        "print(values.shape)\n",
        "\n",
        "# NOTE: The rest of the notebook will use the .tfrecord data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate all embeddings"
      ],
      "metadata": {
        "id": "7zEG-fZiJvzd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE55-FOMcyym"
      },
      "outputs": [],
      "source": [
        "# Generate all the embeddings in .tfrecord format\n",
        "generate_embeddings(input_files=df_labels[\"dicom_file\"].values, output_dir=EMBEDDINGS_DIR,\n",
        "    input_type=InputFileType.DICOM, output_type=OutputFileType.TFRECORD, model_version=MODEL_VERSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXPwZR3wHBEn"
      },
      "outputs": [],
      "source": [
        "# Inspect a tfrecord embedding file. A single file is only several kbs.\n",
        "filename = df_labels[\"embedding_file\"][0]\n",
        "\n",
        "# Read the tf.train.Example object from the first tfrecord file\n",
        "example = embeddings_data.read_tfrecord_example(filename)\n",
        "print(example)\n",
        "\n",
        "# If you don't care about the structure of the .tfrecord file, and/or if\n",
        "# you don't use Tensorflow, you can use the following function to read\n",
        "# the values directly into a numpy array.\n",
        "values = embeddings_data.read_tfrecord_values(filename)\n",
        "print(values.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Efficient Classification Demo"
      ],
      "metadata": {
        "id": "ehcg1l44KY0g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoHZSZO_hDdl"
      },
      "source": [
        "## Prepare Data for Model Training\n",
        "\n",
        "Separate into training, validation, and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr8tsaTLNpEH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "df_train, df_validate = train_test_split(df_labels, test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVGYhxEkWBhs"
      },
      "source": [
        "## Train A Model\n",
        "\n",
        "Finally, we can train a model using the embeddings! With a simple feed-forward neural network, it should take < 5 minutes to train 100 epochs! No GPU required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6KeAAEJcyym"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_models as tfm\n",
        "\n",
        "\n",
        "def create_model(heads,\n",
        "                 # token_num=1 for original CXR foundation embedding\n",
        "                 # token_num=32 for ELIXR embedding\n",
        "                 token_num=1,\n",
        "                 embeddings_size=1376,\n",
        "                 learning_rate=0.1,\n",
        "                 end_lr_factor=1.0,\n",
        "                 dropout=0.0,\n",
        "                 decay_steps=1000,\n",
        "                 loss_weights=None,\n",
        "                 hidden_layer_sizes=[512, 256],\n",
        "                 weight_decay=0.0,\n",
        "                 seed=None) -> tf.keras.Model:\n",
        "  \"\"\"\n",
        "  Creates linear probe or multilayer perceptron using LARS + cosine decay.\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  inputs = tf.keras.Input(shape=(token_num * embeddings_size,))\n",
        "  inputs_reshape = tf.keras.layers.Reshape((token_num, embeddings_size))(inputs)\n",
        "  inputs_pooled = tf.keras.layers.GlobalAveragePooling1D(data_format='channels_last')(inputs_reshape)\n",
        "  hidden = inputs_pooled\n",
        "  # If no hidden_layer_sizes are provided, model will be a linear probe.\n",
        "  for size in hidden_layer_sizes:\n",
        "    hidden = tf.keras.layers.Dense(\n",
        "        size,\n",
        "        activation='relu',\n",
        "        kernel_initializer=tf.keras.initializers.HeUniform(seed=seed),\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(l2=weight_decay),\n",
        "        bias_regularizer=tf.keras.regularizers.l2(l2=weight_decay))(\n",
        "            hidden)\n",
        "    hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
        "    hidden = tf.keras.layers.Dropout(dropout, seed=seed)(hidden)\n",
        "  output = tf.keras.layers.Dense(\n",
        "      units=len(heads),\n",
        "      activation='sigmoid',\n",
        "      kernel_initializer=tf.keras.initializers.HeUniform(seed=seed))(\n",
        "          hidden)\n",
        "\n",
        "  outputs = {}\n",
        "  for i, head in enumerate(heads):\n",
        "    outputs[head] = tf.keras.layers.Lambda(\n",
        "        lambda x: x[..., i:i + 1], name=head.lower())(\n",
        "            output)\n",
        "\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  learning_rate_fn = tf.keras.experimental.CosineDecay(\n",
        "      tf.cast(learning_rate, tf.float32),\n",
        "      tf.cast(decay_steps, tf.float32),\n",
        "      alpha=tf.cast(end_lr_factor, tf.float32))\n",
        "  model.compile(\n",
        "      optimizer=tfm.optimization.lars.LARS(\n",
        "          learning_rate=learning_rate_fn),\n",
        "      loss=dict([(head, 'binary_crossentropy') for head in heads]),\n",
        "      loss_weights=loss_weights or dict([(head, 1.) for head in heads]),\n",
        "      weighted_metrics=[\n",
        "        tf.keras.metrics.FalsePositives(),\n",
        "        tf.keras.metrics.FalseNegatives(),\n",
        "        tf.keras.metrics.TruePositives(),\n",
        "        tf.keras.metrics.TrueNegatives(),\n",
        "        tf.keras.metrics.AUC(),\n",
        "        tf.keras.metrics.AUC(curve='PR', name='auc_pr')])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0UbFRBncyym"
      },
      "outputs": [],
      "source": [
        "# Create training and validation Datasets\n",
        "training_data = embeddings_data.get_dataset(\n",
        "    filenames=df_train[\"embedding_file\"].values,\n",
        "    labels=df_train[DIAGNOSIS].values,\n",
        "    embeddings_size=TOKEN_NUM * EMBEDDINGS_SIZE)\n",
        "\n",
        "\n",
        "validation_data = embeddings_data.get_dataset(\n",
        "    filenames=df_validate[\"embedding_file\"].values,\n",
        "    labels=df_validate[DIAGNOSIS].values,\n",
        "    embeddings_size=TOKEN_NUM * EMBEDDINGS_SIZE)\n",
        "\n",
        "# Create and train the model\n",
        "model = create_model(\n",
        "    [DIAGNOSIS],\n",
        "    token_num=TOKEN_NUM,\n",
        "    embeddings_size = EMBEDDINGS_SIZE,\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    x=training_data.batch(512).prefetch(tf.data.AUTOTUNE).cache(),\n",
        "    validation_data=validation_data.batch(1).cache(),\n",
        "    epochs=100,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUrd2ApY3NVH"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNpdUNuqyWk9"
      },
      "source": [
        "## Examine metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxr52PYuybZp"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_curve(x, y, auc, x_label=None, y_label=None, label=None):\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  plt.plot(x, y, label=f'{label} (AUC: %.3f)' % auc, color='black')\n",
        "  plt.legend(loc='lower right', fontsize=18)\n",
        "  plt.xlim([-0.01, 1.01])\n",
        "  plt.ylim([-0.01, 1.01])\n",
        "  if x_label:\n",
        "    plt.xlabel(x_label, fontsize=24)\n",
        "  if y_label:\n",
        "    plt.ylabel(y_label, fontsize=24)\n",
        "  plt.xticks(fontsize=12)\n",
        "  plt.yticks(fontsize=12)\n",
        "  plt.grid(visible=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6q-A_H39qY3"
      },
      "outputs": [],
      "source": [
        "rows = []\n",
        "for embeddings, label in validation_data.batch(1):\n",
        "  row = {\n",
        "      f'{DIAGNOSIS}_prediction': model(embeddings)[DIAGNOSIS].numpy().flatten()[0],\n",
        "      f'{DIAGNOSIS}_value': label.numpy().flatten()[0]\n",
        "  }\n",
        "  rows.append(row)\n",
        "eval_df = pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXgsYY8emnT8"
      },
      "outputs": [],
      "source": [
        "eval_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eh8MKqF8mdjp"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "labels = eval_df[f'{DIAGNOSIS}_value'].values\n",
        "predictions = eval_df[f'{DIAGNOSIS}_prediction'].values\n",
        "false_positive_rate, true_positive_rate, thresholds = sklearn.metrics.roc_curve(\n",
        "    labels,\n",
        "    predictions,\n",
        "    drop_intermediate=False)\n",
        "auc = sklearn.metrics.roc_auc_score(labels, predictions)\n",
        "plot_curve(false_positive_rate, true_positive_rate, auc, x_label='False Positive Rate', y_label='True Positive Rate', label=DIAGNOSIS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Retrieval Demo"
      ],
      "metadata": {
        "id": "tJIOTPOpKwlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if EMBEDDING_VERSION != 'elixr_img_contrastive':\n",
        "  raise ValueError(\n",
        "      'elixr_img_contrastive embedding is required for image retrieval demo.'\n",
        "  )"
      ],
      "metadata": {
        "id": "w_eC1QjbNZ6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT_QUERY = 'Airspace opacity' #@param {type: \"string\"}"
      ],
      "metadata": {
        "id": "qYH4YooFLZMP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if TEXT_QUERY != 'Airspace opacity':\n",
        "  raise ValueError(\n",
        "      'Since the pool of image is from the above 200 airspace opacity sampling,'\n",
        "      ' the whole image pool might not contain any relevant images other than '\n",
        "      'airspace opacity, but feel free to explore and run the cells in the '\n",
        "      'following sections.'\n",
        "  )"
      ],
      "metadata": {
        "id": "mGfPl9Y0LgRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cxr_foundation.inference import generate_elixr_text_embeddings\n",
        "\n",
        "txt_emb = generate_elixr_text_embeddings(TEXT_QUERY)\n",
        "print('text embedding shape:', txt_emb.shape)"
      ],
      "metadata": {
        "id": "zSiSl7KyNTO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute image-text similarity and display the top 5 images."
      ],
      "metadata": {
        "id": "Y6QEp8OVNp8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_image_text_similarity(image_emb, txt_emb):\n",
        "  image_emb = np.reshape(image_emb, (32, 128))\n",
        "  max_similarity = -1.0\n",
        "  for i in range(32):\n",
        "    # cosine similarity\n",
        "    similarity = np.dot(image_emb[i], txt_emb)/(np.linalg.norm(image_emb[i]) * np.linalg.norm(txt_emb))\n",
        "    if similarity > max_similarity:\n",
        "      max_similarity = similarity\n",
        "  return max_similarity\n",
        "\n",
        "sim_df_key = []\n",
        "sim_df_value = []\n",
        "\n",
        "for _, row in df_labels.iterrows():\n",
        "  filename = row[\"embedding_file\"]\n",
        "  values = embeddings_data.read_tfrecord_values(filename)\n",
        "  similarity = compute_image_text_similarity(values, txt_emb)\n",
        "  sim_df_key.append(row[\"remote_dicom_file\"])\n",
        "  sim_df_value.append(similarity)\n",
        "\n",
        "similarity_df = pd.DataFrame({'remote_dicom_file': sim_df_key, 'similarity': sim_df_value})\n",
        "similarity_df = similarity_df.merge(df_labels, on='remote_dicom_file')\n",
        "similarity_df = similarity_df.sort_values(by=['similarity'], ascending=False)\n",
        "\n",
        "similarity_df"
      ],
      "metadata": {
        "id": "cT-iZF-vNnsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the top 5 images"
      ],
      "metadata": {
        "id": "8BJZq5W-N4lA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  dicom = pydicom.read_file(similarity_df[\"dicom_file\"].values[i])\n",
        "  example = example_generator_lib.dicom_to_tfexample(dicom)\n",
        "  print('Top %d image' % (i+1))\n",
        "  show_dicom(example)"
      ],
      "metadata": {
        "id": "Drqda7MwN2UD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-e9SeKUzTnSL"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "d3ac608b8f9188be2227ae82298dfd5de684cbdc4496f362d4b3b9040509447c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
